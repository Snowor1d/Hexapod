# train_hexapod_global.py
import os
import glob
import multiprocessing as mp
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv, VecMonitor
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.logger import configure
from hexapod_hardware import HexapodEnv
#import terminal_observation
import time

# ===============================================================
# ğŸ”§ GLOBAL CONFIGURATION
# ===============================================================
MODE = "bc_train"   # "teacher", "student_rl", "collect", "bc_train", "bc_eval"

# Paths
# â¬‡ï¸ í™˜ê²½ë³€ìˆ˜ë¡œ XML êµì²´ ê°€ëŠ¥: export HEXAPOD_XML=hexapod.xml
XML_PATH = os.getenv("HEXAPOD_XML", "hexapod_hardware.xml")
LOGDIR   = "./logs_hexapod_hardware"

# Env Params (XMLê³¼ í•©ë§ì¶¤: imu_site/foot_site1~6, keyframe 'home' ì‚¬ìš© ê°€ì •)
N_ENVS = 6
SEED = 42
ACTION_REPEAT = 20
TARGET_SPEED = 0.1
STUDENT_HIST_LEN = 1
CONTACT_THRESHOLD = 1e-4
MAX_STEPS = 500
RENDER = False  # í•„ìš” ì‹œ Trueë¡œ
WARMUP_STEPS = 50
RANDOM_INIT_POSTURE = False

# PPO Params
NET_ARCH = [96, 96]
LR = 3e-4
N_STEPS = 1024
BATCH_SIZE = 1024
N_EPOCHS = 10
GAMMA = 0.99
GAE_LAMBDA = 0.95
CLIP_RANGE = 0.2
ENT_COEF = 0.001
VF_COEF = 0.5
TOTAL_TIMESTEPS = 6_000_000

# Collection & Evaluation
COLLECT_STEPS = 3_000_000
EVAL_STEPS = 3000

# BC Params
BC_HIDDEN = [256, 256]
BC_LR = 1e-3
BC_BATCH = 4096
BC_EPOCHS = 20
BC_VAL_SPLIT = 0.1
FORCE_CPU = False

#Dart params
DART_ENABLED = True
DART_SIGMA = 0.2
DART_CLIP = 0.4

STUDENT_INCLUDE_IMU = True

torch.set_num_threads(4)
os.environ.setdefault("OMP_NUM_THREADS", "4")


# ===============================================================
# ğŸ§© ENV FACTORY
# ===============================================================
def make_env(rank, obs_mode):
    """
    obs_mode: "teacher" | "student" | "teacher_student"
    """
    def _init():
        env = HexapodEnv(
            xml_path=XML_PATH,
            render_mode="none",
            action_repeat=ACTION_REPEAT,
            target_speed=TARGET_SPEED,
            obs_clip=10.0,
            seed=SEED + rank,
            obs_mode=obs_mode,
            student_hist_len=STUDENT_HIST_LEN,
            contact_threshold=CONTACT_THRESHOLD,
            max_steps=MAX_STEPS,
            warmup_steps=WARMUP_STEPS,
            random_init_posture = RANDOM_INIT_POSTURE,
            student_include_imu = STUDENT_INCLUDE_IMU
        )
        return Monitor(env)
    return _init


def build_ppo(venv, logdir):
    model = PPO(
        "MlpPolicy",
        venv,
        policy_kwargs=dict(net_arch=list(NET_ARCH)),
        learning_rate=LR,
        n_steps=N_STEPS,
        batch_size=BATCH_SIZE,
        n_epochs=N_EPOCHS,
        gamma=GAMMA,
        gae_lambda=GAE_LAMBDA,
        clip_range=CLIP_RANGE,
        ent_coef=ENT_COEF,
        vf_coef=VF_COEF,
        verbose=1,
    )
    logger = configure(logdir, ["tensorboard", "stdout"])
    model.set_logger(logger)
    return model


# ===============================================================
# ğŸ§  TRAIN TEACHER
# ===============================================================
def train_teacher():
    run_dir = os.path.join(LOGDIR, "teacher")
    os.makedirs(run_dir, exist_ok=True)
    if N_ENVS > 1:
        venv = SubprocVecEnv([make_env(i, "teacher") for i in range(N_ENVS)])
    else:
        venv = DummyVecEnv([make_env(0, "teacher")])
    venv = VecMonitor(venv, filename=os.path.join(run_dir, "monitor.csv"))

    model = build_ppo(venv, run_dir)
    model.learn(total_timesteps=TOTAL_TIMESTEPS)
    model.save(os.path.join(run_dir, "ppo_teacher"))
    venv.close()


# ===============================================================
# ğŸ§  STUDENT RL
# ===============================================================
def train_student_rl():
    run_dir = os.path.join(LOGDIR, "student_rl")
    os.makedirs(run_dir, exist_ok=True)
    if N_ENVS > 1:
        venv = SubprocVecEnv([make_env(i, "student") for i in range(N_ENVS)])
    else:
        venv = DummyVecEnv([make_env(0, "student")])
    venv = VecMonitor(venv, filename=os.path.join(run_dir, "monitor.csv"))

    model = build_ppo(venv, run_dir)
    model.learn(total_timesteps=TOTAL_TIMESTEPS)
    model.save(os.path.join(run_dir, "ppo_student_rl"))
    venv.close()

# ===============================================================
# ğŸ¯ COLLECT (teacher â†’ student pairs) [ìˆ˜ì •ë¨: ë³‘ë ¬ ì²˜ë¦¬]
# ===============================================================
def collect_teacher_student_pairs():
    run_dir = os.path.join(LOGDIR, "collect")
    os.makedirs(run_dir, exist_ok=True)

    # ğŸ”¹ ë‹¨ì¼ í™˜ê²½ ì‚¬ìš© (vecenv ì•ˆ ì”€)
    env = HexapodEnv(
        xml_path=XML_PATH,
        render_mode="none",
        action_repeat=ACTION_REPEAT,
        target_speed=TARGET_SPEED,
        obs_clip=10.0,
        seed=SEED,
        obs_mode="teacher_student",      # teacher obs + info["student_obs"]
        student_hist_len=STUDENT_HIST_LEN,
        contact_threshold=CONTACT_THRESHOLD,
        max_steps=MAX_STEPS,
        warmup_steps=WARMUP_STEPS,
        student_include_imu = STUDENT_INCLUDE_IMU
        # random_init_posture=True,      # ëœë¤ ì´ˆê¸°ìì„¸ ì“°ê³  ì‹¶ìœ¼ë©´ ì¼œê¸°
    )

    teacher_path = os.path.join(LOGDIR, "teacher", "ppo_teacher.zip")
    if not os.path.exists(teacher_path):
        raise FileNotFoundError(f"Teacher policy not found: {teacher_path}")
    from stable_baselines3 import PPO
    teacher = PPO.load(teacher_path)

    S_list, A_list = [], []
    obs, info = env.reset()
    ep_step = 0

    for t in range(COLLECT_STEPS):
        # ğŸ”¹ studentê°€ ì‹¤ì œë¡œ ë³´ê²Œ ë  ê´€ì¸¡
        if(t%100000 ==0):
            print(t, " collected")
        student_obs_t = info["student_obs"].copy()

        # ğŸ”¹ teacherëŠ” teacher_obs(=obs)ë¥¼ ë³´ê³  í–‰ë™
        action, _ = teacher.predict(obs, deterministic=False)

        # ğŸ”¹ warmup ì´í›„ êµ¬ê°„ë§Œ BC ë°ì´í„°ì— ë„£ê¸°
        if ep_step >= WARMUP_STEPS:
            S_list.append(student_obs_t)
            A_list.append(action.copy())

        # ğŸ”¹ DART: noisy actionìœ¼ë¡œ í™˜ê²½ì„ êµ´ë¦¬ê³ ,
        #     labelì€ clean teacher actionìœ¼ë¡œ ìœ ì§€
        if DART_ENABLED:
            noise = np.random.normal(0.0, DART_SIGMA, size=action.shape).astype(np.float32)
            noise = np.clip(noise, -DART_CLIP, DART_CLIP)
            step_action = np.clip(action + noise, -1.0, 1.0)
        else:
            step_action = action

        obs, r, done, trunc, info = env.step(step_action)
        ep_step += 1

        if done or trunc:
            obs, info = env.reset()
            ep_step = 0

    env.close()

    S = np.stack(S_list, axis=0).astype(np.float32)
    A = np.stack(A_list, axis=0).astype(np.float32)
    out = os.path.join(
        run_dir,
        f"pairs_S{S.shape[-1]}_A{A.shape[-1]}_N{S.shape[0]}.npz"
    )
    np.savez_compressed(out, S=S, A=A)
    print(f"[collect] Saved {S.shape[0]} samples -> {out}")

# ===============================================================
# ğŸ¤– BEHAVIOR CLONING (BC)
# ===============================================================
class StudentDataset(Dataset):
    def __init__(self, files, validation_split=0.1, mode="train"):
        Ss, As = [], []
        for f in files:
            d = np.load(f)
            Ss.append(d["S"]); As.append(d["A"])
        
        self.S = np.concatenate(Ss, 0).astype(np.float32)
        self.A = np.concatenate(As, 0).astype(np.float32)
        
        # 1. í›ˆë ¨/ê²€ì¦ ì¸ë±ìŠ¤ ë¶„í• 
        num_samples = self.S.shape[0]
        indices = np.arange(num_samples)
        np.random.shuffle(indices) # ë°ì´í„° ì„ê¸°
        
        split_idx = int(num_samples * (1 - validation_split))
        
        if mode == "train":
            self.indices = indices[:split_idx]
        elif mode == "val":
            self.indices = indices[split_idx:]
        else:
            raise ValueError(f"Unknown mode: {mode}")

    def __len__(self): 
        return len(self.indices) # ë¶„í• ëœ ì¸ë±ìŠ¤ ê°œìˆ˜ ë°˜í™˜
    
    def __getitem__(self, i): 
        idx = self.indices[i] # ë¶„í• ëœ ì¸ë±ìŠ¤ ì‚¬ìš©
        return self.S[idx], self.A[idx]


class MLP(nn.Module):
    def __init__(self, in_dim, out_dim, hidden=(256, 256)):
        super().__init__()
        layers, last = [], in_dim
        for h in hidden:
            layers += [nn.Linear(last, h), nn.ReLU()]
            last = h
        layers += [nn.Linear(last, out_dim), nn.Tanh()]
        self.net = nn.Sequential(*layers)
    def forward(self, x): return self.net(x)

def bc_train():
    collect_dir = os.path.join(LOGDIR, "collect")
    files = sorted(glob.glob(os.path.join(collect_dir, "*.npz")))
    if not files:
        raise FileNotFoundError("No collected .npz found in ./logs_hexapod_hardware/collect")
    
    # 2. í›ˆë ¨/ê²€ì¦ ë°ì´í„°ì…‹ ìƒì„±
    train_ds = StudentDataset(files, validation_split=BC_VAL_SPLIT, mode="train")
    val_ds = StudentDataset(files, validation_split=BC_VAL_SPLIT, mode="val")
    
    Sdim, Adim = train_ds.S.shape[1], train_ds.A.shape[1]
    print(f"[BC Train] Train N={len(train_ds)}, Val N={len(val_ds)}, Sdim={Sdim}, Adim={Adim}")

    # 3. ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(train_ds, batch_size=BC_BATCH, shuffle=True, drop_last=True, num_workers=4)
    val_loader = DataLoader(val_ds, batch_size=BC_BATCH, shuffle=False, drop_last=False, num_workers=4)
    
    dev = torch.device("cuda" if torch.cuda.is_available() and not FORCE_CPU else "cpu")
    net = MLP(Sdim, Adim, BC_HIDDEN).to(dev)
    opt = torch.optim.Adam(net.parameters(), lr=BC_LR)
    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=BC_EPOCHS)

    outdir = os.path.join(LOGDIR, "student_bc")
    os.makedirs(outdir, exist_ok=True)
    best_val_loss = float("inf")
    ckpt = os.path.join(outdir, "student_bc.pt")

    # ğŸ”¹ ì—¬ê¸°ì„œë¶€í„°: loss íˆìŠ¤í† ë¦¬ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
    train_history = []
    val_history = []

    for ep in range(1, BC_EPOCHS + 1):
        # --- í›ˆë ¨ ---
        loss_sum = 0.0
        net.train()
        for S, A in train_loader:
            S, A = S.to(dev), A.to(dev)
            pred = net(S)
            loss = ((pred - A) ** 2).mean()
            opt.zero_grad()
            loss.backward()
            opt.step()
            loss_sum += loss.item()
        
        avg_train_loss = loss_sum / len(train_loader)
        
        # --- ê²€ì¦ ---
        val_loss_sum = 0.0
        net.eval()
        with torch.no_grad():
            for S_val, A_val in val_loader:
                S_val, A_val = S_val.to(dev), A_val.to(dev)
                pred_val = net(S_val)
                val_loss = ((pred_val - A_val) ** 2).mean()
                val_loss_sum += val_loss.item()
        
        avg_val_loss = val_loss_sum / len(val_loader)
        sched.step()
        
        # ğŸ”¹ íˆìŠ¤í† ë¦¬ì— ì €ì¥
        train_history.append(avg_train_loss)
        val_history.append(avg_val_loss)

        print(f"[BC] epoch {ep}/{BC_EPOCHS} "
              f"Train_MSE={avg_train_loss:.6f}  Val_MSE={avg_val_loss:.6f}")
        
        # ê²€ì¦ ì†ì‹¤ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(
                {"model": net.state_dict(), "Sdim": Sdim, "Adim": Adim, "hidden": BC_HIDDEN},
                ckpt
            )
            print(f"  â†³ saved best ckpt (Val_MSE={best_val_loss:.6f}) -> {ckpt}")

    # ğŸ”¹ í•™ìŠµ ëë‚œ í›„ loss ê¸°ë¡ ì €ì¥ (npz)
    hist_path = os.path.join(outdir, "bc_loss_history.npz")
    np.savez_compressed(
        hist_path,
        train=np.array(train_history, dtype=np.float32),
        val=np.array(val_history, dtype=np.float32),
    )
    print(f"[BC] Saved loss history -> {hist_path}")


def bc_eval():
    ckpt = os.path.join(LOGDIR, "student_bc", "student_bc.pt")
    if not os.path.exists(ckpt):
        raise FileNotFoundError("No BC checkpoint at logs_hexapod_hardware/student_bc/student_bc.pt")
    d = torch.load(ckpt, map_location="cpu")
    net = MLP(d["Sdim"], d["Adim"], d["hidden"])
    net.load_state_dict(d["model"])
    dev = torch.device("cuda" if torch.cuda.is_available() and not FORCE_CPU else "cpu")
    net.to(dev).eval()

    # 6. make_env íŒ©í† ë¦¬ ì‚¬ìš© (ì¼ê´€ì„±)
    # RENDER=Trueì´ë©´ rank=0ì´ë¯€ë¡œ human ëª¨ë“œë¡œ ìë™ ì„¤ì •ë¨
    env = make_env(0, "student", use_monitor=False)()

    obs, _ = env.reset()
    ret, steps = 0.0, 0
    with torch.no_grad():
        for _ in range(EVAL_STEPS):
            S = torch.from_numpy(obs).unsqueeze(0).to(dev)
            a = net(S).cpu().numpy()[0]
            obs, r, done, trunc, info = env.step(a)
            ret += r
            steps += 1
            if RENDER: # ë Œë”ë§ ì‹œ ì•½ê°„ì˜ ë”œë ˆì´
                time.sleep(0.01)
            if done or trunc:
                print(f"[BC Eval] Return={ret:.2f} Steps={steps}")
                ret, steps = 0.0, 0
                obs, _ = env.reset()
    env.close()

# ===============================================================
# ğŸš€ MAIN
# ===============================================================
if __name__ == "__main__":
    # MacOSì—ì„œ SubprocVecEnv ì‚¬ìš© ì‹œ í•„ìˆ˜
    mp.set_start_method("spawn", force=True)
    os.makedirs(LOGDIR, exist_ok=True)

    if MODE == "teacher":
        train_teacher()
    elif MODE == "student_rl":
        train_student_rl()
    elif MODE == "collect":
        collect_teacher_student_pairs()
    elif MODE == "bc_train":
        bc_train()
    elif MODE == "bc_eval":
        bc_eval()
    else:
        raise ValueError(f"Unknown MODE: {MODE}")
